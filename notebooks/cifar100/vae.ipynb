{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/davideleo/Desktop/Projects/research/papers/fl_wavelet_v0\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch \n",
    "import numpy as np \n",
    "from src.data.cifar100 import get_federation \n",
    "from src.federated_learning.standard.fedavg import Client as Client\n",
    "from src.models.neural_networks import LeNet5\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "# Federation\n",
    "model = LeNet5(in_channels = 3, in_padding = 0, num_classes = 100)\n",
    "\n",
    "test_dataset = get_federation(\n",
    "    num_shards = 1,\n",
    "    alpha = 1000,\n",
    "    attacks = [],\n",
    "    attacks_proba = 0\n",
    ")[0][\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Train VAE \n",
    "num_features = 200\n",
    "num_epochs = 100 \n",
    "batch_size = 64 \n",
    "device = \"cpu\"\n",
    "\n",
    "gradients = []\n",
    "test_dataset.load()\n",
    "\n",
    "dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "model = model.to(device)\n",
    "optim = Adam(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)): \n",
    "    for X, y in dataloader: \n",
    "        theta0_sd = model.cpu().state_dict()[\"_mlp.5.weight\"].clone()\n",
    "        y_hat = model(X.to(device))\n",
    "        loss = criterion(y_hat, y.to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        dtheta = model.cpu().state_dict()[\"_mlp.5.weight\"] - theta0_sd\n",
    "        flat = dtheta.flatten()\n",
    "        indices = torch.randperm(flat.size(0))[:num_features]\n",
    "        sampled = flat[indices].clone()\n",
    "        gradients.append(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Loss: 6.0630\n",
      "Epoch 2/100 - Loss: 2.6886\n",
      "Epoch 3/100 - Loss: 1.5033\n",
      "Epoch 4/100 - Loss: 1.0817\n",
      "Epoch 5/100 - Loss: 0.8609\n",
      "Epoch 6/100 - Loss: 0.7351\n",
      "Epoch 7/100 - Loss: 0.6233\n",
      "Epoch 8/100 - Loss: 0.5426\n",
      "Epoch 9/100 - Loss: 0.4697\n",
      "Epoch 10/100 - Loss: 0.4195\n",
      "Epoch 11/100 - Loss: 0.3640\n",
      "Epoch 12/100 - Loss: 0.3193\n",
      "Epoch 13/100 - Loss: 0.2839\n",
      "Epoch 14/100 - Loss: 0.2485\n",
      "Epoch 15/100 - Loss: 0.2232\n",
      "Epoch 16/100 - Loss: 0.1995\n",
      "Epoch 17/100 - Loss: 0.1767\n",
      "Epoch 18/100 - Loss: 0.1587\n",
      "Epoch 19/100 - Loss: 0.1452\n",
      "Epoch 20/100 - Loss: 0.1309\n",
      "Epoch 21/100 - Loss: 0.1183\n",
      "Epoch 22/100 - Loss: 0.1079\n",
      "Epoch 23/100 - Loss: 0.0982\n",
      "Epoch 24/100 - Loss: 0.0895\n",
      "Epoch 25/100 - Loss: 0.0824\n",
      "Epoch 26/100 - Loss: 0.0753\n",
      "Epoch 27/100 - Loss: 0.0698\n",
      "Epoch 28/100 - Loss: 0.0645\n",
      "Epoch 29/100 - Loss: 0.0590\n",
      "Epoch 30/100 - Loss: 0.0552\n",
      "Epoch 31/100 - Loss: 0.0507\n",
      "Epoch 32/100 - Loss: 0.0473\n",
      "Epoch 33/100 - Loss: 0.0437\n",
      "Epoch 34/100 - Loss: 0.0411\n",
      "Epoch 35/100 - Loss: 0.0380\n",
      "Epoch 36/100 - Loss: 0.0358\n",
      "Epoch 37/100 - Loss: 0.0336\n",
      "Epoch 38/100 - Loss: 0.0313\n",
      "Epoch 39/100 - Loss: 0.0292\n",
      "Epoch 40/100 - Loss: 0.0276\n",
      "Epoch 41/100 - Loss: 0.0260\n",
      "Epoch 42/100 - Loss: 0.0243\n",
      "Epoch 43/100 - Loss: 0.0230\n",
      "Epoch 44/100 - Loss: 0.0218\n",
      "Epoch 45/100 - Loss: 0.0205\n",
      "Epoch 46/100 - Loss: 0.0193\n",
      "Epoch 47/100 - Loss: 0.0182\n",
      "Epoch 48/100 - Loss: 0.0174\n",
      "Epoch 49/100 - Loss: 0.0164\n",
      "Epoch 50/100 - Loss: 0.0158\n",
      "Epoch 51/100 - Loss: 0.0148\n",
      "Epoch 52/100 - Loss: 0.0141\n",
      "Epoch 53/100 - Loss: 0.0133\n",
      "Epoch 54/100 - Loss: 0.0127\n",
      "Epoch 55/100 - Loss: 0.0121\n",
      "Epoch 56/100 - Loss: 0.0115\n",
      "Epoch 57/100 - Loss: 0.0109\n",
      "Epoch 58/100 - Loss: 0.0104\n",
      "Epoch 59/100 - Loss: 0.0099\n",
      "Epoch 60/100 - Loss: 0.0096\n",
      "Epoch 61/100 - Loss: 0.0091\n",
      "Epoch 62/100 - Loss: 0.0087\n",
      "Epoch 63/100 - Loss: 0.0083\n",
      "Epoch 64/100 - Loss: 0.0080\n",
      "Epoch 65/100 - Loss: 0.0076\n",
      "Epoch 66/100 - Loss: 0.0073\n",
      "Epoch 67/100 - Loss: 0.0071\n",
      "Epoch 68/100 - Loss: 0.0068\n",
      "Epoch 69/100 - Loss: 0.0065\n",
      "Epoch 70/100 - Loss: 0.0062\n",
      "Epoch 71/100 - Loss: 0.0060\n",
      "Epoch 72/100 - Loss: 0.0058\n",
      "Epoch 73/100 - Loss: 0.0055\n",
      "Epoch 74/100 - Loss: 0.0053\n",
      "Epoch 75/100 - Loss: 0.0051\n",
      "Epoch 76/100 - Loss: 0.0049\n",
      "Epoch 77/100 - Loss: 0.0048\n",
      "Epoch 78/100 - Loss: 0.0046\n",
      "Epoch 79/100 - Loss: 0.0044\n",
      "Epoch 80/100 - Loss: 0.0042\n",
      "Epoch 81/100 - Loss: 0.0041\n",
      "Epoch 82/100 - Loss: 0.0040\n",
      "Epoch 83/100 - Loss: 0.0038\n",
      "Epoch 84/100 - Loss: 0.0037\n",
      "Epoch 85/100 - Loss: 0.0035\n",
      "Epoch 86/100 - Loss: 0.0034\n",
      "Epoch 87/100 - Loss: 0.0034\n",
      "Epoch 88/100 - Loss: 0.0032\n",
      "Epoch 89/100 - Loss: 0.0031\n",
      "Epoch 90/100 - Loss: 0.0030\n",
      "Epoch 91/100 - Loss: 0.0029\n",
      "Epoch 92/100 - Loss: 0.0028\n",
      "Epoch 93/100 - Loss: 0.0027\n",
      "Epoch 94/100 - Loss: 0.0026\n",
      "Epoch 95/100 - Loss: 0.0026\n",
      "Epoch 96/100 - Loss: 0.0025\n",
      "Epoch 97/100 - Loss: 0.0024\n",
      "Epoch 98/100 - Loss: 0.0023\n",
      "Epoch 99/100 - Loss: 0.0023\n",
      "Epoch 100/100 - Loss: 0.0022\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from src.models.neural_networks import VAE\n",
    "\n",
    "class TensorListDataset(Dataset):\n",
    "    def __init__(self, tensor_list):\n",
    "        self.data = tensor_list \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Loss function\n",
    "def vae_loss_function(recon_x, x, mu, logvar):\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "# Setup\n",
    "num_epochs = 100 \n",
    "batch_size = 128 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = TensorListDataset(gradients)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "model = VAE(num_features).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(batch)\n",
    "        loss = vae_loss_function(recon_batch, batch, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(dataloader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"notebooks/cifar100/results/vae.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
